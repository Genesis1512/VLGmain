{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8919f5-9365-4152-87cf-21513f913c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5d4357-3623-453c-9b0c-74b7ef77b362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n",
      "Validation Dataset: <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "MAX_TRAIN_IMAGES = 350\n",
    "\n",
    "\n",
    "def data_build(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocessing(low_light_images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n",
    "    dataset = dataset.map(data_build, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "trainig_data = sorted(glob(\"./Train/low/*\"))[:MAX_TRAIN_IMAGES]\n",
    "valid_data = sorted(glob(\"./Train/low/*\"))[MAX_TRAIN_IMAGES: 417]\n",
    "test_data = sorted(glob(\"./Train/low/*\"))[418 :]\n",
    "#test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n",
    "\n",
    "\n",
    "train= preprocessing(trainig_data)\n",
    "val_dataset = preprocessing(valid_data)\n",
    "\n",
    "print(\"Train Dataset:\", train)\n",
    "print(\"Validation Dataset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c0417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossanalysis(X, num_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    return cluster_labels, kmeans.cluster_centers_\n",
    "\n",
    "def losspredict():\n",
    "    data = request.get_json(force=True)\n",
    "    features = np.array(data['features']).reshape(1, -1)\n",
    "    prediction = model.predict(features)\n",
    "    return jsonify({'prediction': prediction.tolist()})\n",
    "\n",
    "def lossoptimization(estimator, param_bounds, X_train, y_train, cv=5):\n",
    "    def objective_function(**params):\n",
    "        estimator.set_params(**params)\n",
    "        scores = cross_val_score(estimator, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function,\n",
    "        pbounds=param_bounds,\n",
    "        random_state=42,\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    best_params = optimizer.max['params']\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b8f101-9610-4a7a-acee-a02971849a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "    image = keras.Input(shape=[None, None, 3])\n",
    "    a1 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(image)\n",
    "    a2 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(a1)\n",
    "    a3 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(a2)\n",
    "    a4 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(a3)\n",
    "    int_con1 = layers.Concatenate(axis=-1)([a4, a3])\n",
    "    a5 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(int_con1)\n",
    "    int_con2 = layers.Concatenate(axis=-1)([a5, a2])\n",
    "    a6 = layers.Conv2D(\n",
    "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
    "    )(int_con2)\n",
    "    int_con3 = layers.Concatenate(axis=-1)([a6, a1])\n",
    "    result = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n",
    "        int_con3\n",
    "    )\n",
    "    return keras.Model(inputs=image, outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88809be9-bf40-4f3e-b04e-4abbe6f6563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorloss(x):\n",
    "    mean_score = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n",
    "    mean_red, mean_green, mean_blue = (\n",
    "        mean_score[:, :, :, 0],\n",
    "        mean_score[:, :, :, 1],\n",
    "        mean_score[:, :, :, 2],\n",
    "    )\n",
    "    delta_red_green = tf.square(mean_red - mean_green)\n",
    "    delta_green_blue = tf.square(mean_blue - mean_green)\n",
    "    delta_blue_red =  tf.square(mean_blue - mean_red)\n",
    "    return tf.sqrt(tf.square(delta_red_green) + tf.square(delta_blue_red) + tf.square(delta_green_blue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f776f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossbalance(X, y):\n",
    "    print(f'Original class distribution: {Counter(y)}')\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    print(f'Resampled class distribution: {Counter(y_resampled)}')\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def ilosscheck(X_train, y_train):\n",
    "    estimators = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "        ('svc', SVC(random_state=42))\n",
    "    ]\n",
    "    stack_model = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LogisticRegression()\n",
    "    )\n",
    "    stack_model.fit(X_train, y_train)\n",
    "    return stack_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc07c93-f63e-4533-8ec4-cb681f5eb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_loss(x, mean_val=0.6):\n",
    "    x = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n",
    "    return tf.reduce_mean(tf.square(mean - mean_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ae8f33-defb-45c9-a146-ce8130a1ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossinsmoothness(x):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    height = tf.shape(x)[1]\n",
    "    width = tf.shape(x)[2]\n",
    "    height_count = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n",
    "    width_count = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n",
    "    hori_variation = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : height - 1, :, :])))\n",
    "    verti_variation = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : width - 1, :])))\n",
    "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
    "    height_count = tf.cast(height_count, dtype=tf.float32)\n",
    "    width_count = tf.cast(width_count, dtype=tf.float32)\n",
    "    return 2 * (hori_variation / height_count + verti_variation / width_count) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b0a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def losscount(model, X, index):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(X, mode='regression')\n",
    "    explanation = explainer.explain_instance(X[index], model.predict, num_features=len(X[index]))\n",
    "    return explanation\n",
    "\n",
    "def smoothness(series, order=(1, 1, 1)):\n",
    "    model = ARIMA(series, order=order)\n",
    "    fitted_model = model.fit()\n",
    "    return fitted_model\n",
    "\n",
    "def ikcolor(model, steps=10):\n",
    "    forecast = model.forecast(steps=steps)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f112c3-3964-4344-9b73-828aad033f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialConsistencyLoss(keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(reduction=\"none\")\n",
    "\n",
    "        self.left_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.right_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.up_kernel = tf.constant(\n",
    "            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "        self.down_kernel = tf.constant(\n",
    "            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def call(self, true_image, predicted_image):\n",
    "        original_mean = tf.reduce_mean(true_image, 3, keepdims=True)\n",
    "        enhanced_mean = tf.reduce_mean(predicted_image, 3, keepdims=True)\n",
    "        original_pool = tf.nn.avg_pool2d(\n",
    "            original_mean, ksize=4, strides=4, padding=\"VALID\"\n",
    "        )\n",
    "        enhanced_pool = tf.nn.avg_pool2d(\n",
    "            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n",
    "        )\n",
    "\n",
    "        original_left = tf.nn.conv2d(\n",
    "            original_pool,\n",
    "            self.left_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        original_right = tf.nn.conv2d(\n",
    "            original_pool,\n",
    "            self.right_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        original_up = tf.nn.conv2d(\n",
    "            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        original_down = tf.nn.conv2d(\n",
    "            original_pool,\n",
    "            self.down_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "\n",
    "        enhanced_left = tf.nn.conv2d(\n",
    "            enhanced_pool,\n",
    "            self.left_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        enhanced_right = tf.nn.conv2d(\n",
    "            enhanced_pool,\n",
    "            self.right_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "        enhanced_up = tf.nn.conv2d(\n",
    "            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    "        )\n",
    "        enhanced_down = tf.nn.conv2d(\n",
    "            enhanced_pool,\n",
    "            self.down_kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"SAME\",\n",
    "        )\n",
    "\n",
    "        left = tf.square(original_left - enhanced_left)\n",
    "        right = tf.square(original_right - enhanced_right)\n",
    "        up = tf.square(original_up - enhanced_up)\n",
    "        down = tf.square(original_down - enhanced_down)\n",
    "        return left + right + up + down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d1a84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(X_train, y_train):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Feature importance\")\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), indices)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "def ikdata(X):\n",
    "    imputer = IterativeImputer(random_state=0)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    return X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd769032-c09c-4532-b704-de56502406da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class themodel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dce_model = basic_model()\n",
    "\n",
    "    def compile(self, learning_rate, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.lossinsmoothness_tracker = keras.metrics.Mean(\n",
    "            name=\"lossinsmoothness\"\n",
    "        )\n",
    "        self.spatial_constancy_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"spatial_constancy_loss\"\n",
    "        )\n",
    "        self.colorloss_tracker = keras.metrics.Mean(\n",
    "            name=\"colorloss\"\n",
    "        )\n",
    "        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.lossinsmoothness_tracker,\n",
    "            self.spatial_constancy_loss_tracker,\n",
    "            self.colorloss_tracker,\n",
    "            self.exposure_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def get_enhanced_image(self, data, output):\n",
    "        enhancement_layer1 = output[:, :, :, :3]\n",
    "        enhancement_layer2 = output[:, :, :, 3:6]\n",
    "        enhancement_layer3 = output[:, :, :, 6:9]\n",
    "        enhancement_layer4 = output[:, :, :, 9:12]\n",
    "        enhancement_layer5 = output[:, :, :, 12:15]\n",
    "        enhancement_layer6 = output[:, :, :, 15:18]\n",
    "        enhancement_layer7 = output[:, :, :, 18:21]\n",
    "        enhancement_layer8 = output[:, :, :, 21:24]\n",
    "        intermediate_img = data + enhancement_layer1 * (tf.square(data) - data)\n",
    "        intermediate_img = intermediate_img + enhancement_layer2 * (tf.square(intermediate_img) - intermediate_img)\n",
    "        intermediate_img = intermediate_img + enhancement_layer3 * (tf.square(intermediate_img) - intermediate_img)\n",
    "        enhanced_image = intermediate_img + enhancement_layer4 * (tf.square(intermediate_img) - intermediate_img)\n",
    "        intermediate_img = enhanced_image + enhancement_layer5 * (tf.square(enhanced_image) - enhanced_image)\n",
    "        intermediate_img = intermediate_img + enhancement_layer6 * (tf.square(intermediate_img) - intermediate_img)\n",
    "        intermediate_img = intermediate_img + enhancement_layer7 * (tf.square(intermediate_img) - intermediate_img)\n",
    "        enhanced_image = intermediate_img + enhancement_layer8 * (tf.square(intermediate_img) - intermediate_img)\n",
    "        return enhanced_image\n",
    "\n",
    "    def call(self, data):\n",
    "        dce_net_output = self.dce_model(data)\n",
    "        return self.get_enhanced_image(data, dce_net_output)\n",
    "\n",
    "    def compute_losses(self, data, output):\n",
    "        enhanced_image = self.get_enhanced_image(data, output)\n",
    "        loss_illumination = 200 * lossinsmoothness(output)\n",
    "        loss_spatial_constancy = tf.reduce_mean(\n",
    "            self.spatial_constancy_loss(enhanced_image, data)\n",
    "        )\n",
    "        loss_color_constancy = 5 * tf.reduce_mean(colorloss(enhanced_image))\n",
    "        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n",
    "        total_loss = (\n",
    "            loss_illumination\n",
    "            + loss_spatial_constancy\n",
    "            + loss_color_constancy\n",
    "            + loss_exposure\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"lossinsmoothness\": loss_illumination,\n",
    "            \"spatial_constancy_loss\": loss_spatial_constancy,\n",
    "            \"colorloss\": loss_color_constancy,\n",
    "            \"exposure_loss\": loss_exposure,\n",
    "        }\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.dce_model(data)\n",
    "            losses = self.compute_losses(data, output)\n",
    "\n",
    "        gradients = tape.gradient(\n",
    "            losses[\"total_loss\"], self.dce_model.trainable_weights\n",
    "        )\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
    "        self.lossinsmoothness_tracker.update_state(\n",
    "            losses[\"lossinsmoothness\"]\n",
    "        )\n",
    "        self.spatial_constancy_loss_tracker.update_state(\n",
    "            losses[\"spatial_constancy_loss\"]\n",
    "        )\n",
    "        self.colorloss_tracker.update_state(losses[\"colorloss\"])\n",
    "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
    "\n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        output = self.dce_model(data)\n",
    "        losses = self.compute_losses(data, output)\n",
    "\n",
    "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
    "        self.lossinsmoothness_tracker.update_state(\n",
    "            losses[\"lossinsmoothness\"]\n",
    "        )\n",
    "        self.spatial_constancy_loss_tracker.update_state(\n",
    "            losses[\"spatial_constancy_loss\"]\n",
    "        )\n",
    "        self.colorloss_tracker.update_state(losses[\"colorloss\"])\n",
    "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
    "\n",
    "        return {metric.name: metric.result() for metric in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a7bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87754643-12d4-420c-8fff-29cf461b9aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 8s/step - colorloss: 0.0046 - exposure_loss: 3.0131 - lossinsmoothness: 2.9645 - spatial_constancy_loss: 7.6098e-06 - total_loss: 5.9822 - val_colorloss: 0.0016 - val_exposure_loss: 2.9310 - val_lossinsmoothness: 4.0599 - val_spatial_constancy_loss: 2.6613e-06 - val_total_loss: 6.9925\n",
      "Epoch 2/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 8s/step - colorloss: 0.0038 - exposure_loss: 2.9985 - lossinsmoothness: 1.8824 - spatial_constancy_loss: 3.3764e-06 - total_loss: 4.8847 - val_colorloss: 0.0016 - val_exposure_loss: 2.9202 - val_lossinsmoothness: 2.9802 - val_spatial_constancy_loss: 7.8729e-06 - val_total_loss: 5.9019\n",
      "Epoch 3/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 10s/step - colorloss: 0.0037 - exposure_loss: 2.9891 - lossinsmoothness: 1.3487 - spatial_constancy_loss: 9.5205e-06 - total_loss: 4.3415 - val_colorloss: 0.0016 - val_exposure_loss: 2.9098 - val_lossinsmoothness: 2.2869 - val_spatial_constancy_loss: 1.9065e-05 - val_total_loss: 5.1983\n",
      "Epoch 4/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 9s/step - colorloss: 0.0038 - exposure_loss: 2.9794 - lossinsmoothness: 1.0167 - spatial_constancy_loss: 2.1815e-05 - total_loss: 3.9999 - val_colorloss: 0.0016 - val_exposure_loss: 2.8998 - val_lossinsmoothness: 1.8184 - val_spatial_constancy_loss: 3.5261e-05 - val_total_loss: 4.7198\n",
      "Epoch 5/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 9s/step - colorloss: 0.0038 - exposure_loss: 2.9703 - lossinsmoothness: 0.7973 - spatial_constancy_loss: 3.7932e-05 - total_loss: 3.7715 - val_colorloss: 0.0017 - val_exposure_loss: 2.8896 - val_lossinsmoothness: 1.4867 - val_spatial_constancy_loss: 5.6292e-05 - val_total_loss: 4.3780\n",
      "Epoch 6/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 9s/step - colorloss: 0.0039 - exposure_loss: 2.9609 - lossinsmoothness: 0.6460 - spatial_constancy_loss: 5.8200e-05 - total_loss: 3.6109 - val_colorloss: 0.0017 - val_exposure_loss: 2.8777 - val_lossinsmoothness: 1.2396 - val_spatial_constancy_loss: 8.5155e-05 - val_total_loss: 4.1191\n",
      "Epoch 7/100\n",
      "\u001b[1m14/21\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 9s/step - colorloss: 0.0041 - exposure_loss: 2.9740 - lossinsmoothness: 0.4818 - spatial_constancy_loss: 7.8415e-05 - total_loss: 3.4599"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m zero_dce_model \u001b[38;5;241m=\u001b[39m themodel()\n\u001b[1;32m      5\u001b[0m zero_dce_model\u001b[38;5;241m.\u001b[39mcompile(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[psnr])\n\u001b[0;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m zero_dce_model\u001b[38;5;241m.\u001b[39mfit(train, validation_data\u001b[38;5;241m=\u001b[39mval_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_result\u001b[39m(obj_pic):\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[obj_pic], label\u001b[38;5;241m=\u001b[39mobj_pic)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    328\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 329\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    330\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    331\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def psnr(true_image, predicted_image):\n",
    "    return tf.image.psnr(true_image, predicted_image, max_val=1.0)\n",
    "\n",
    "zero_dce_model = themodel()\n",
    "zero_dce_model.compile(learning_rate=1e-4, metrics=[psnr])\n",
    "history = zero_dce_model.fit(train, validation_data=val_dataset, epochs=100)\n",
    "def plot_result(obj_pic):\n",
    "    plt.plot(history.history[obj_pic], label=obj_pic)\n",
    "    plt.plot(history.history[\"val_\" + obj_pic], label=\"val_\" + obj_pic)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(obj_pic)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(obj_pic), fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"total_loss\")\n",
    "plot_result(\"lossinsmoothness\")\n",
    "plot_result(\"spatial_constancy_loss\")\n",
    "plot_result(\"colorloss\")\n",
    "plot_result(\"exposure_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(images, titles, figure_size=(12, 12)):\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(len(images)):\n",
    "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
    "        _ = plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def infer(original_image):\n",
    "    image = keras.utils.img_to_array(original_image)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    output_image = zero_dce_model(image)\n",
    "    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n",
    "    output_image = Image.fromarray(output_image.numpy())\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a35873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_with_weights(models, weights, X_train, y_train):\n",
    "    estimators = [(f'model_{i}', model) for i, model in enumerate(models)]\n",
    "    ensemble_model = VotingClassifier(estimators=estimators, voting='soft', weights=weights)\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    return ensemble_model\n",
    "\n",
    "def classification(X_train, y_train):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('svm', SVC(kernel='linear'))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66696466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calculate_mae(original_image, enhanced_image):\n",
    "    # Mean Absolute Error calculation\n",
    "    original_array = np.array(original_image)\n",
    "    output_array = np.array(enhanced_image)\n",
    "    return np.mean(np.abs(original_array - output_array))\n",
    "\n",
    "def calculate_mse(original_image, enhanced_image):\n",
    "    # Mean Squared Error calculation\n",
    "    original_array = np.array(original_image)\n",
    "    output_array = np.array(enhanced_image)\n",
    "    return np.mean((original_array - output_array) ** 2)\n",
    "\n",
    "def calculate_psnr(original_image, enhanced_image):\n",
    "    # Peak Signal-to-Noise Ratio (PSNR) calculation assuming pixel values range from 0 to 255\n",
    "    mse = calculate_mse(original_image, enhanced_image)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    pixel_max = 255.0\n",
    "    return 20 * math.log10(pixel_max / math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a90736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_loss_svm(X):\n",
    "    svm = OneClassSVM(kernel='rbf', nu=0.01)\n",
    "    anomalies = svm.fit_predict(X)\n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52eaf7-7e0a-4a51-928b-6d8a095f533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= preprocessing(test_data)\n",
    "print(\"Train Dataset:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d439e138-b969-4782-98e5-ddba1f7a5c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import display\n",
    "\n",
    "total_mae = 0.0\n",
    "total_mse = 0.0\n",
    "total_psnr = 0.0\n",
    "\n",
    "\n",
    "for idx, val_image_file in enumerate(test_data):\n",
    "    original_image = Image.open(val_image_file)\n",
    "    enhanced_image = infer(original_image)\n",
    "    plot_results(\n",
    "        [original_image,enhanced_image],\n",
    "        [\"Provided\", \"Enhanced\"],\n",
    "        (20, 12),\n",
    "    )\n",
    "    mae = calculate_mae(original_image,enhanced_image)\n",
    "    mse = calculate_mse(original_image, enhanced_image)\n",
    "    psnr = calculate_psnr(original_image, enhanced_image)\n",
    "    total_mae += mae\n",
    "    total_mse += mse\n",
    "    total_psnr += psnr\n",
    "    cwd = os.getcwd()\n",
    "    save_dir = os.path.join(cwd, 'predicted')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(val_image_file))[0]\n",
    "    cv2_output_filename = f\"{base_filename}_cv2_{idx}.jpg\"\n",
    "    pil_output_filename = f\"{base_filename}_pil_{idx}.jpg\"\n",
    "    outputphoto_np = np.array(enhanced_image)\n",
    "    cv2.imwrite(os.path.join(save_dir, cv2_output_filename), cv2.cvtColor(outputphoto_np, cv2.COLOR_RGB2BGR))\n",
    "    outputphoto_pil = Image.fromarray(outputphoto_np)\n",
    "    outputphoto_pil.save(os.path.join(save_dir, pil_output_filename))\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Peak Signal-to-Noise Ratio (PSNR): {psnr} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(test_data)\n",
    "avg_mae = total_mae / num_images\n",
    "avg_mse = total_mse / num_images\n",
    "avg_psnr = total_psnr / num_images\n",
    "\n",
    "print(f\"Average MAE across {num_images} images: {avg_mae}\")\n",
    "print(f\"Average MSE across {num_images} images: {avg_mse}\")\n",
    "print(f\"Average PSNR across {num_images} images: {avg_psnr} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4bc00-3d07-432b-af4f-284fbf21d27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
